If we have two random variablesÂ ğ‘‹Â andÂ ğ‘ŒÂ we can define the covariance of two random variablesÂ ğ‘‹Â andÂ ğ‘Œ,Â $ğ¶ğ‘œğ‘£(ğ‘‹,ğ‘Œ)$Â as:

$$ğ‘ğ‘œğ‘£(ğ‘‹,ğ‘Œ)=ğ¸[(ğ‘‹âˆ’ğ‘¥)(ğ‘Œâˆ’ğ‘¦)]$$

where Î¼x and Î¼y are the means ofÂ ğ‘‹Â and Y, respectively. The above can be written as:

$$ğ‘ğ‘œğ‘£(ğ‘‹,ğ‘Œ)=ğ¸[(ğ‘‹âˆ’ğ¸[ğ‘‹])(ğ‘Œâˆ’ğ¸[ğ‘Œ])]=ğ¸[ğ‘‹ğ‘Œ]âˆ’ğ¸[ğ‘‹]ğ¸[ğ‘Œ]$$

From its definition, we see that covariance satisfies the following properties:

$$ğ¶ğ‘œğ‘£(ğ‘‹,ğ‘Œ)=ğ¶ğ‘œğ‘£(ğ‘Œ,ğ‘‹)$$

and

$$ğ¶ğ‘œğ‘£(ğ‘‹,ğ‘‹)=ğ‘‰ğ‘ğ‘Ÿ(ğ‘‹)$$

Another property of covariance, which immediately follows from its definition, is that for any constantÂ ğ‘:

$$ğ¶ğ‘œğ‘£(ğ‘ğ‘‹,ğ‘Œ)=ğ‘ğ¶ğ‘œğ‘£(ğ‘‹,ğ‘Œ)$$

ifÂ ğ‘‹ andÂ ğ‘ŒÂ are independent random variables, then

$$ğ¶ğ‘œğ‘£(ğ‘‹,ğ‘Œ)=0$$

and so for independentÂ $ğ‘‹1,...,ğ‘‹ğ‘›$

$$ğ‘‰ğ‘ğ‘Ÿ(\sum_{ğ‘–=1}^ğ‘›ğ‘‹ğ‘–)=\sum_{ğ‘–=1}^ğ‘›ğ‘‰ğ‘ğ‘Ÿ(ğ‘‹ğ‘–)$$