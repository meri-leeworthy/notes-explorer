TheÂ **expected value of a constant**Â is the constant itself. $ğ¸(ğ‘)=ğ‘Â ;Â ğ‘$ is any constant.Â This can be verified by noting that:

$$ğ¸(ğ‘)=\int_{âˆ’âˆ}^âˆğ‘ğ‘“(ğ‘¥)ğ‘‘ğ‘¥=ğ‘\int_{âˆ’âˆ}^âˆğ‘“(ğ‘¥)ğ‘‘ğ‘¥$$

and by definition:

$$\int_{âˆ’âˆ}^âˆğ‘“(ğ‘¥)ğ‘‘ğ‘¥=1$$

Therefore, the expected value of a constant times a random variable is the constant times the expected value of the random variable:

$$ğ¸[ğ‘ğ‘‹]=ğ‘ğ¸[ğ‘‹]$$

whereÂ ğ‘Â is any constant with respect toÂ ğ‘‹.

This can also be verified by:

$$ğ¸[ğ‘ğ‘‹]=\int_{âˆ’âˆ}^âˆğ‘ğ‘¥ğ‘“(ğ‘¥)ğ‘‘ğ‘¥=ğ‘\int_{âˆ’âˆ}^âˆğ‘¥ğ‘“(ğ‘¥)ğ‘‘ğ‘¥=ğ‘ğ¸[ğ‘‹]$$

The expected value of two terms is the sum of the expected value of each.

$$ğ¸[ğ‘‹+ğ‘Œ]=ğ¸[ğ‘‹]+ğ¸[ğ‘Œ]$$

$$ğ¸[ğ‘“(ğ‘¥)+ğ‘”(ğ‘¥)]=ğ¸[ğ‘“(ğ‘¥)]+ğ¸[ğ‘”(ğ‘¥)]$$

The expected value of a random variableÂ ğ‘‹Â is also called theÂ **mean ofÂ ğ‘‹**Â and is often designated byÂ Î¼. The expected value of $(ğ‘‹â€“Î¼)^2$Â is called the variance ofÂ ğ‘‹. The positive square root of the variance is called the standard deviation. The termsÂ $Ïƒ^2$Â andÂ $Ïƒ$Â (sigma squared and sigma) represent the variance and standard deviation, respectively.Â **Variance**Â is a measure of the spread or dispersion of the values of the random variable about its mean value. 

TheÂ **standard deviation**Â isÂ $\sqrt{ğ‘‰[ğ‘‹]}$, which is also a measure of spread or dispersion. The standard deviation is expressed in the same units asÂ ğ‘‹, whereas the variance is expressed in the square of these units.