[[Module 5_ Topic 3 â€“ Regression _ MonashELMS1.pdf]]
#### Regression

In regression, our task is to predict a continuous value.Â For example, the price of a car.

![](../public/74e27d00ea10cc303f513d0d25624d5d.png)

To do this, we might use a feature like top speed.

![](../public/ee9eb351db150383fa004e58b0c76336.png)

Or we might want to 'try' to predict stock prices from other continuous attributes like company profit, debt, and cash flow.

![](../public/e564f8309675682b8ff528de4ba46462.png)

- ExampleÂ ğ‘¦ğ‘—Â is stock price,Â ğ‘¥ğ‘—Â contains company profit, debt, cash flow, . . .
- Given a training setÂ (ğ‘¥1,ğ‘¦1),...,(ğ‘¥ğ‘,ğ‘¦ğ‘)
- We assumeÂ ğ‘¥ğ‘—âˆˆâ„ğ‘š,ğ‘¦ğ‘—âˆˆâ„

From here on, we'll assume our data is continuous.

### 1.2. Univariate linear regression

#### Univariate linear regression

Given a training set of N continuous valued pairs:Â (ğ‘¥1,ğ‘¦1),...,(ğ‘¥ğ‘,ğ‘¦ğ‘),Â we would like to learn a linear function with two coefficients to predict y from x.Â 

We want to learn the parameters w0 and w1 such that:  

â„ğ‘¥(ğ‘¥)=ğ‘¤1ğ‘¥+ğ‘¤0

  

The loss function for our model is the sum over all N data points of the square of the difference between our predicted value for y and the true value of y.

**Loss:**Â Square of the difference between the trueÂ ğ‘¦ğ‘—Â and predicted target valueÂ (ğ‘¤1ğ‘¥ğ‘—+ğ‘¤0)Â forÂ ğ‘¥ğ‘—:

ğ¿ğ‘œğ‘ ğ‘ (â„ğ‘¤)=âˆ‘ğ‘—=1ğ‘(ğ‘¦ğ‘—âˆ’â„ğ‘¤(ğ‘¥ğ‘—))2=âˆ‘ğ‘—=1ğ‘(ğ‘¦ğ‘—âˆ’(ğ‘¤1ğ‘¥ğ‘—+ğ‘¤0))2

So, for linear regression for a modelÂ â„ğ‘¤â„ğ‘¤Â trained on a data set:

ğ·={(ğ±1,ğ‘¦1),â€¦,(ğ±ğ‘,ğ‘¦ğ‘)},ğ±ğ‘—âˆˆâ„ğ‘š,ğ‘¦ğ‘—âˆˆâ„

the loss function is:

ğ¿ğ‘œğ‘ ğ‘ (â„ğ‘¤)=âˆ‘ğ‘—=1ğ‘(ğ‘¦ğ‘—âˆ’â„ğ‘¤(ğ‘¥ğ‘—))2

This loss function is called theÂ **sum of squares error (SSE)**Â orÂ **residual sum of squares (RSS)**, and can also be applied as a loss function to other regression algorithms.Â 

##### Assessment tip: Other loss functions for regression

We may also use other loss functions for regression. Two loss functions that you will encounter in Assessment 3 are theÂ **mean absolute error (MAE)**:

ğ‘€ğ´ğ¸(â„ğ‘¤)=1ğ‘âˆ‘ğ‘—=1ğ‘âˆ£âˆ£ğ‘¦ğ‘—âˆ’â„ğ‘¤(ğ±ğ‘—)âˆ£âˆ£ğ‘€ğ´ğ¸(â„ğ‘¤)=1ğ‘âˆ‘ğ‘—=1ğ‘|ğ‘¦ğ‘—âˆ’â„ğ‘¤(ğ‘¥ğ‘—)|  

andÂ **root mean squared error (RMSE)**:

ğ‘…ğ‘€ğ‘†ğ¸(â„ğ‘¤)=1ğ‘âˆ‘ğ‘—=1ğ‘(ğ‘¦ğ‘—âˆ’â„ğ‘¤(ğ±ğ‘—))2â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆš


#### Learning the parameters

To learn the parameters, we would like to find the coefficientsÂ ğ‘¤âˆ—=(ğ‘¤âˆ—0,ğ‘¤âˆ—1)ğ‘¤âˆ—=(ğ‘¤0âˆ—,ğ‘¤1âˆ—)Â that minimise the loss functionÂ ğ¿ğ‘œğ‘ ğ‘ (â„ğ‘¤)ğ¿ğ‘œğ‘ ğ‘ (â„ğ‘¤)Â on the training data.

We can do this by finding the derivative of the loss function and setting it to 0.Â   

- ğ¿ğ‘œğ‘ ğ‘ (â„ğ‘¤)ğ¿ğ‘œğ‘ ğ‘ (â„ğ‘¤)Â is a convex function, soÂ ğ‘¤âˆ—ğ‘¤âˆ—Â is unique
- FindÂ ğ‘¤âˆ—ğ‘¤âˆ—Â by settingÂ the derivative to 0.

âˆ‚âˆ‚ğ‘¤ğ‘–ğ¿ğ‘œğ‘ ğ‘ (â„ğ‘¤)=0âˆ‚âˆ‚ğ‘¤ğ‘–ğ¿ğ‘œğ‘ ğ‘ (â„ğ‘¤)=0

Then we solve the values forÂ ğ‘¤ğ‘¤Â with anÂ **iterative algorithm**Â like gradient descent, though our simple linear functionÂ â„ğ‘¤=ğ‘¤1ğ‘¥+ğ‘¤0â„ğ‘¤=ğ‘¤1ğ‘¥+ğ‘¤0Â has an analytic solution with the given formulas we can use directly to find the values forÂ ğ‘¤0ğ‘¤0Â andÂ ğ‘¤1ğ‘¤1.  

ğ‘¤0=âˆ‘ğ‘¦ğ‘—âˆ’ğ‘¤1âˆ‘ğ‘¥ğ‘—ğ‘ğ‘¤0=âˆ‘ğ‘¦ğ‘—âˆ’ğ‘¤1âˆ‘ğ‘¥ğ‘—ğ‘

  

ğ‘¤1=ğ‘âˆ‘ğ‘¥ğ‘—ğ‘¦ğ‘—âˆ’âˆ‘ğ‘¥ğ‘—âˆ‘ğ‘¦ğ‘—ğ‘âˆ‘ğ‘¥2ğ‘—âˆ’(âˆ‘ğ‘¥ğ‘—)2


#### Example

Computing the analytic solution in practice requires the computation of a slow matrix inverse unless the data size is small. When data size is large,Â **gradient descent**Â is preferred.

This example uses analytic formulas. With the 3 data points given, we compute the values shown forÂ ğ‘¤1ğ‘¤1Â andÂ ğ‘¤0ğ‘¤0.

- Training data:Â (1,3),(2.1,0.5),(âˆ’5,6.2)(1,3),(2.1,0.5),(âˆ’5,6.2)
- Linear regression model:Â â„ğ‘¤=ğ‘¤1ğ‘¥+ğ‘¤0â„ğ‘¤=ğ‘¤1ğ‘¥+ğ‘¤0

##### ForÂ ğ‘¤1ğ‘¤1

ğ‘¤1=3(1Ã—3+2.1Ã—0.5+(âˆ’5)Ã—6.2)âˆ’(1+2.1âˆ’5)(3+0.5+6.2)3(12+2.12+(âˆ’5)2)âˆ’(1+2.1âˆ’5)2=âˆ’0.712ğ‘¤1=3(1Ã—3+2.1Ã—0.5+(âˆ’5)Ã—6.2)âˆ’(1+2.1âˆ’5)(3+0.5+6.2)3(12+2.12+(âˆ’5)2)âˆ’(1+2.1âˆ’5)2=âˆ’0.712

##### ForÂ ğ‘¤0ğ‘¤0

ğ‘¤0=(3+0.5+6.2)âˆ’ğ‘¤1(1+2.1âˆ’5)3=2.78ğ‘¤0=(3+0.5+6.2)âˆ’ğ‘¤1(1+2.1âˆ’5)3=2.78  

---

We can also plot the 3 data points and the solution we found and see how it fits the data.

- Training data:Â (1,3),(2.1,0.5),(âˆ’5,6.2)(1,3),(2.1,0.5),(âˆ’5,6.2)
- Linear regression model:Â â„ğ‘¤=âˆ’0.712ğ‘¥+2.78

![](../public/ef76edbccb7166a97816198ca4dd27ed.png)

### 1.3. Gradient descent

#### Gradient descent

The fundamental idea of theÂ **gradient descent algorithm**Â is that we iteratively move towards a function minimum by using the gradient to tell us the direction of the steepest descent and take small steps in that direction.Â 

The algorithm picks an initial value of the coefficientsÂ ğ‘¤0ğ‘¤0Â and iteratively moves each coefficient towards the minimum by following the negative direction of the gradient until the valuesÂ **converge**.Â 

![](../public/9d64dd4ee1271ffcf345cbfed234f4ea.png)

- ğ°0ğ‘¤0Â â† any point in the parameter space;Â ğ‘¡=1ğ‘¡=1
- Until convergenceÂ (||ğ°ğ‘¡âˆ’ğ‘šğ‘ğ‘¡â„ğ‘ğ‘“ğ‘¤ğ‘¡âˆ’1||<Ïµ)(||ğ‘¤ğ‘¡âˆ’ğ‘šğ‘ğ‘¡â„ğ‘ğ‘“ğ‘¤ğ‘¡âˆ’1||<Ïµ)

- For eachÂ ğ‘¤ğ‘–ğ‘¤ğ‘–Â inÂ ğ°:ğ‘¤ğ‘¡ğ‘–â†ğ‘¤ğ‘¡âˆ’1ğ‘–âˆ’ğ›¼âˆ‚âˆ‚ğ‘¤ğ‘–ğ¿ğ‘œğ‘ ğ‘ (â„ğ‘¤)=0ğ‘¤:ğ‘¤ğ‘–ğ‘¡â†ğ‘¤ğ‘–ğ‘¡âˆ’1âˆ’ğ›¼âˆ‚âˆ‚ğ‘¤ğ‘–ğ¿ğ‘œğ‘ ğ‘ (â„ğ‘¤)=0

In the algorithm given,Â ğ›¼ğ›¼Â represents a learning rate constant that adjusts how much the algorithm updates the coefficients at each step.


#### Gradient descent: Univariate linear regression

InÂ **univariate linear regression,**Â we have the following identities for the gradients concerningÂ ğ‘¤0ğ‘¤0Â andÂ ğ‘¤1ğ‘¤1.

âˆ‚âˆ‚ğ‘¤0ğ¿ğ‘œğ‘ ğ‘ (â„ğ‘¤)=âˆ’2(ğ‘¦âˆ’â„ğ‘¤(ğ‘¥))âˆ‚âˆ‚ğ‘¤1ğ¿ğ‘œğ‘ ğ‘ (â„ğ‘¤)=âˆ’2(ğ‘¦âˆ’â„ğ‘¤(ğ‘¥))ğ‘¥âˆ‚âˆ‚ğ‘¤0ğ¿ğ‘œğ‘ ğ‘ (â„ğ‘¤)=âˆ’2(ğ‘¦âˆ’â„ğ‘¤(ğ‘¥))âˆ‚âˆ‚ğ‘¤1ğ¿ğ‘œğ‘ ğ‘ (â„ğ‘¤)=âˆ’2(ğ‘¦âˆ’â„ğ‘¤(ğ‘¥))ğ‘¥

We have the corresponding update rules for one training exampleÂ (ğ‘¥ğ‘–,ğ‘¥ğ‘—)(ğ‘¥ğ‘–,ğ‘¥ğ‘—)

ğ‘¤ğ‘¡0âŸµğ‘¤ğ‘¡âˆ’10+ğ›¼(ğ‘¦ğ‘—âˆ’â„ğ‘¤(ğ‘¥ğ‘—))ğ‘¤ğ‘¡1âŸµğ‘¤ğ‘¡âˆ’11+ğ›¼(ğ‘¦ğ‘—âˆ’â„ğ‘¤(ğ‘¥ğ‘—))ğ‘¥ğ‘—ğ‘¤0ğ‘¡âŸµğ‘¤0ğ‘¡âˆ’1+ğ›¼(ğ‘¦ğ‘—âˆ’â„ğ‘¤(ğ‘¥ğ‘—))ğ‘¤1ğ‘¡âŸµğ‘¤1ğ‘¡âˆ’1+ğ›¼(ğ‘¦ğ‘—âˆ’â„ğ‘¤(ğ‘¥ğ‘—))ğ‘¥ğ‘—

We also have a data set with N examples.

ğ‘¤ğ‘¡0âŸµğ‘¤ğ‘¡âˆ’10+Î±âˆ‘ğ‘—=1ğ‘(ğ‘¦ğ‘—âˆ’â„ğ‘¤(ğ‘¥ğ‘—))ğ‘¤ğ‘¡1âŸµğ‘¤ğ‘¡âˆ’11+Î±âˆ‘ğ‘—=1ğ‘(ğ‘¦ğ‘—âˆ’â„ğ‘¤(ğ‘¥ğ‘—))ğ‘¥ğ‘—ğ‘¤0ğ‘¡âŸµğ‘¤0ğ‘¡âˆ’1+Î±âˆ‘ğ‘—=1ğ‘(ğ‘¦ğ‘—âˆ’â„ğ‘¤(ğ‘¥ğ‘—))ğ‘¤1ğ‘¡âŸµğ‘¤1ğ‘¡âˆ’1+Î±âˆ‘ğ‘—=1ğ‘(ğ‘¦ğ‘—âˆ’â„ğ‘¤(ğ‘¥ğ‘—))ğ‘¥ğ‘—

Note that the 2 from the derivative is combined with the learning rate alpha as a single constant.


#### Gradient descent: Multivariable linear regression

Univariate linear regression is unlikely to be the case in a real-world example. It is far more likely that weâ€™ll have many features we want to predict from and need to performÂ **multivariate linear regression**.

We need a new coefficient for each newÂ ğ‘¥ğ‘¥Â feature we add.

â„ğ‘¤(ğ‘¥ğ‘—)=ğ‘¤0+ğ‘¤1ğ‘¥ğ‘—,1+...+ğ‘¤ğ‘šğ‘¥ğ‘—,ğ‘š=ğ‘¤0+âˆ‘ğ‘–=1ğ‘šğ‘¤ğ‘–ğ‘¥ğ‘—,ğ‘–â„ğ‘¤(ğ‘¥ğ‘—)=ğ‘¤0+ğ‘¤1ğ‘¥ğ‘—,1+...+ğ‘¤ğ‘šğ‘¥ğ‘—,ğ‘š=ğ‘¤0+âˆ‘ğ‘–=1ğ‘šğ‘¤ğ‘–ğ‘¥ğ‘—,ğ‘–

If we define a value forÂ ğ‘¥0=1ğ‘¥0=1Â for all data points, however, then we can write the linear function succinctly as a dot prediction between our feature vectorÂ ğ‘¥ğ‘¥Â and the coefficient vectorÂ ğ‘¤ğ‘¤.

- SetÂ ğ‘¥ğ‘—,0=1ğ‘¥ğ‘—,0=1Â for all samples
- â„ğ‘¤(ğ‘¥ğ‘—)=ğ‘¤â‹…ğ‘¥=âˆ‘ğ‘–=0ğ‘šğ‘¤ğ‘–ğ‘¥ğ‘—,ğ‘–â„ğ‘¤(ğ‘¥ğ‘—)=ğ‘¤â‹…ğ‘¥=âˆ‘ğ‘–=0ğ‘šğ‘¤ğ‘–ğ‘¥ğ‘—,ğ‘–

The gradient descent update rules are also unchanged by adding more features.

- Updates for one training exampleÂ (ğ‘¥ğ‘–,ğ‘¥ğ‘—)(ğ‘¥ğ‘–,ğ‘¥ğ‘—)

ğ‘¤ğ‘¡ğ‘–âŸµğ‘¤ğ‘¡âˆ’1ğ‘–+Î±(ğ‘¦ğ‘—âˆ’â„ğ‘¤(ğ‘¥ğ‘—))ğ‘¥ğ‘—,ğ‘–ğ‘¤ğ‘–ğ‘¡âŸµğ‘¤ğ‘–ğ‘¡âˆ’1+Î±(ğ‘¦ğ‘—âˆ’â„ğ‘¤(ğ‘¥ğ‘—))ğ‘¥ğ‘—,ğ‘–

- Updates for N training examples

ğ‘¤ğ‘¡ğ‘–âŸµğ‘¤ğ‘¡âˆ’1ğ‘–+Î±âˆ‘ğ‘—=1ğ‘(ğ‘¦ğ‘—âˆ’â„ğ‘¤(ğ‘¥ğ‘—))ğ‘¥ğ‘—,ğ‘–