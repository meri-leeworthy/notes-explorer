#### MLE for Gaussian distribution

Letâ€™s attempt the MLE for the Gaussian distribution. Suppose 3 data points were generated from a process described by a Gaussian distribution. These points are 3, 4, and 5. How do we calculate the maximum likelihood estimates of the parameter values of the Gaussian distributionÂ Î¼ andÂ Ïƒ?

We want to calculate the total probability of observing all of the data, that is, the joint probability distribution of all observed data points. To do this, we would need to calculate some conditional probabilities, which can get very difficult.Â 

**Assumption**: Each data point is generated independently of the others.Â 

Assuming that each data point is generated independently of the others makes the maths easier. Recall that forÂ **independent events**, the total probability of observing all of the data is the product of observing each data point individually, which is the product of the marginal probabilities. The probability density of observing a single data pointÂ ğ‘¥, that is generated from a Gaussian distribution is given by:

$$ğ‘ƒ(ğ‘¥;ğœ‡,ğœ)=\frac{1}{ğœ\sqrt{2ğœ‹}}ğ‘’ğ‘¥ğ‘\left(âˆ’\frac{(ğ‘¥âˆ’ğœ‡)^2}{2ğœ^2}\right)$$

---

#### Solving for parameterÂ Î¼Â 

In using MLE, the following steps apply:

##### Step 1

Write the total joint probability density of observing the three data points:

$$ğ‘ƒ(3,4,5;ğœ‡,ğœ)=\frac{1}{ğœ\sqrt{2\pi}}ğ‘’ğ‘¥ğ‘\left(âˆ’\frac{(3âˆ’ğœ‡)^2}{2ğœ^2}\right)
Ã—
\frac{1}{ğœ\sqrt{2\pi}}ğ‘’ğ‘¥ğ‘\left(âˆ’\frac{(4âˆ’ğœ‡)^2}{2ğœ^2}\right)
Ã—\frac{1}{ğœ\sqrt{2\pi}}ğ‘’ğ‘¥ğ‘\left(âˆ’\frac{(5âˆ’ğœ‡)^2}{2ğœ^2|}\right) $$

##### Step 2

Take the natural logarithm of the expression above

$$ğ‘™ğ‘›(ğ‘ƒ(ğ‘¥;ğœ‡,ğœ)=ğ‘™ğ‘›(\frac{1}{ğœ\sqrt{2\pi}})âˆ’\frac{(3âˆ’ğœ‡)^2}{2ğœ^2}+ğ‘™ğ‘›(\frac{1}{ğœ\sqrt{2\pi}})âˆ’\frac{(4âˆ’ğœ‡)^2}{2ğœ^2}+ğ‘™ğ‘›(\frac{1}{ğœ\sqrt{2\pi}})-\frac{(5âˆ’ğœ‡)^2}{2ğœ^2}$$

$$ğ‘™ğ‘›(ğ‘ƒ(ğ‘¥;ğœ‡,ğœ)âˆ’âˆ’3ğ‘™ğ‘›(ğœ)âˆ’\frac{3}{2}ğ‘™ğ‘›(2ğœ‹)âˆ’\frac{1}{2ğœ^2}[(3âˆ’ğœ‡)2+(4âˆ’ğœ‡)2=(5âˆ’ğœ‡)2]$$

##### Step 3

Next, differentiate the natural logarithm expression above concerning parameterÂ ğœ‡ğœ‡Â and set the partial derivatives toÂ 00.

$$\frac{âˆ‚ğ‘™ğ‘›(ğ‘ƒ(ğ‘¥;ğœ‡,ğœ))}{âˆ‚ğœ‡}=\frac{1}{ğœ^2}[3+4+5=3ğœ‡]$$

##### Step 4

Finally, solve for parameter.

$$ğœ‡=\frac{3+4+5}{3}=4$$

---

#### Solving for parameterÂ ğœ

The same steps apply to solve for parameterÂ ğœ, but remember to differentiate the natural logarithm expression concerningÂ ğœ.

A more general form of the maximum likelihood estimation is as below. It can be applied to any random variable, assuming it is independent and identically distributed.

##### Step 1

Letâ€™s say there is a parametric statistical model with parameter(s)Â Î¸, whereÂ $ğ‘ƒ(ğ‘¥|ğœƒ)$Â is the probability density function of a random variable.

##### Step 2

Next, calculate the joint probability of the random variableÂ ğ‘¥, i.e.,Â $ğ‘¥1,â€¦,ğ‘¥ğ‘›$, which is the product of the marginal probabilities of each data pointÂ ğ‘¥.

$$ğ‘ƒ(ğ‘¥|ğœƒ)=\prod^{ğ‘›}_{ğ‘–=1}ğ‘(ğ‘¥_ğ‘–|ğœƒ)$$

##### Step 3

Then perform the negative log-likelihood

$$ğ‘™ğ‘›(ğ‘ƒ(ğ‘¥|ğœƒ))=âˆ’\sum^{ğ‘›}_{ğ‘–=1}ğ‘™ğ‘›(ğ‘(ğ‘¥ğ‘–|ğœƒ))$$

##### Step 4

Next, differentiate concerningÂ Î¸Â and set the partial derivative toÂ 0.

$$\frac{âˆ‚ğ‘™ğ‘›(ğ‘ƒ(ğ‘¥|ğœƒ))}{âˆ‚ğœƒ}=0$$

##### Step 5

Finally, solve forÂ Î¸.